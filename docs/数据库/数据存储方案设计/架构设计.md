# 数据库架构设计

## 前言

技术描述上可能不同，但是架构模式可能相同

1. 模块一：后台业务系统的架构模式，为什么不同业务类型的后台架构是相似的，所有系统其实就CRUD
2. 三种业务类型
   1. 偏向于读业务的高性能和高可用，保证99.99%
      1. 查询短视频信息
      2. 搜索信息，搜索商品信息
      3. 咨询类业务
   2. 偏向于写业务的高性能和高可用，关系到企业的收入，所以要保证101%
      1. 订单业写入，
      2. 各类数据信息
   3. 偏向于扣减业务的系统，要保证并发情况下的数据一致性，以及扛住高并发的能力
      1. 商品库存的扣减
      2. 例如：秒杀业务

## 架构拆分

1. 拆分架构，降低系统架构的复杂性

2. 何时拆分，为何拆分，理由如下

   1. 需要并行开发

   1. 原来代码比较复杂，开发一个需求，每次都要于读原来代码，

   1. 如果需求上线前，要召集上百人开会，浪费人员精力

3. 如何拆分

   1. 一般优先业务流程（垂直）拆分：用户、商品、订单、评论
   2. 再进行分层（水平）拆分：技术拆分：业务逻辑、数据操作


## 高性能读架构

### 业务特点

1. 高可用，故障率要低
2. 高性能，响应要快且支持超高的Qps

### 设计注意事项

1. 对于很高的读性能与物场景，通常使用Redis作为读服务，MySql作为兜底服务
2. 架构尽量不要分层，原因如下
   1. 架构分层后，网络传输比不分层架构时间多了很多
   2. 读业务逻辑比较简单，性能主要消耗在网络传输中

3. CDN将数据放入到距离你最近的机房
4. 注意事项
   1. 尽量避免引入框架
   2. 按需打印日志
   3. 读取内容时，按字段进行读取




### 常见问题

1. 数据一致性问题，不推荐使用分布式事务，极大影响性能，懒加载适用于90%的情况
2. 缓存问题
3. 懒加载（先更新数据库，然后被动更新缓存）仍存在问题
   1. 性能毛刺（请求数据库和缓存）响应的时间存在很大差异，比如首页数据内容不适合懒加载，如果同时有很多人访问数据，最终请求还是会到数据库
   2. 雪崩问题: 过期时间加随机时间
   3. 击穿问题: 布隆过滤器或者优先设置null并返回
   4. 穿透问题：限流添加分布式锁




### 全量缓存设计

使用全量缓存打造高可用读缓存服务（100MS以内响应时间），但是需要的代价就是资源成本的上升，比如内存条

1. 将所有数据放内存中，同时不设置过期时间
2. 但是会产生更新数据上面的分布式事务问题
   1. 解决方案：采用订阅数据库Binlog实现数据同步，获取数据库的所有变更，直接写入到缓存中
3. 开源工具主要有：Canal、MySql-Stream、MaxWell、DataBus
   1. 降低延迟：主从同步保持在毫秒级别
   2. 解决分布式事务问题：Binlog主从复制，基于ACK机制
   3. 提升了代码的简洁性和可维护性：只要维持数据库的表结构不变更，对binlog数据的处理程序就能保持固定
4. 全量缓存问题点
   1. 系统复杂性变高：增加了新的中间件
   2. 缓存容量增加迅速，解决方案
      1. 对缓存数据进行筛选：有业务含义，且被查询到的数据进行缓存
      2. 数据压缩，但是会增加CPU损耗
         1. 使用hash结构
         2. 使用短标识代替长的标识
         3. 异步校准+自动化校准（不推荐-会导致Mysql性能变差）
   3. MySql主从和Binlog协议同步区别
      1. 主从是纯数据同步，性能损耗极低，而Binlog需要经过协议转换，有一性能损耗
      2. binlog整体链路较长，会有一些延迟
      3. 为了保证稳定性，Binlog同步的是从库的，所以延迟进一步加大
      4. binlog是串行的，所以吞吐量会比较低
   4. 如何对于binlog格式解析
   5. 如何保证数据不丢失或错误？
   6. 如何设计缓存数据结构
5. 问题解决方案
   1. binlog模式使用statement模式（最简单，只记录Sql），  row（最全面，记录了变更前后额所有数据信息，无需回表查数据），mix（根据Sql自行判断日志记录的方式）
   2. 并行同步方案：按库进行binlog，ACK后将Binlog直接放入到消息队列中，然后并发消费消息，但是同一条数据可能会有脏数据的问题
      1. 分布式锁
      2. 依赖Mq的串行通道功能，同一条数据放入到同一条MQ中，按顺性执行
         1. 按表设置不同的topic，然后串行消费
      3. 数据不一致问题
         1. 数据对比系统（定时轮训对比数据，如果不一致，设置延时，然后再次对比，增加报警，然后保存旧数据（方便问题定位），最后刷新缓存
      4. 主动写入缓存，保证无延迟，但是可能会丢失数据，而binlog同步方案作为补充，虽然有延迟，保证了数据的一致性

### 缓存热点数据

#### 概念

难点：百万级别的QPS集中在同一条数据上面

类似场景：

1. 热点微博
2. 秒杀商品

#### 设计方案

方案一：主从复制，但是极其资源浪费，且不具备扩展性

方案二：使用应用类的前置缓存，LRU算法，并设置容量，淘汰最旧的数据，然后设置刷新（定时或者主动）

1. 但是可能会导致前置缓存没有数据，逃逸请求全部访问缓存，然后将缓存挂掉
2. 所以方案是限流，只允许一部分请求到缓存中
3. 对于可能超过当前机器承载数量的QPS进行限流，避免前置缓存也挂掉



## 高性能写架构

### 概念

并发百万的写服务

难点：数据的随时写入，保证101%可写入

### 关于分库分表

数据量太大，根据需要决定是否分库或者分表

1. 优先采用分表的方案 
2. 其次采用分库的方案
   1. 按照用户ID
   2. 根据商品的类别

### 架构设计

1. 维持一个可写入数据的数据库列表
2. 按照规则写入进行写入数据，为了保证可以读到已经挂掉的数据库数据，所以需要做binlog数据库备份
3. 为了避免备份出现延迟，可以是用兜底服务，即：主动写入备份数据库，binlog同作为兜底服务

### 高性能

1. 并行化处理
2. 依赖后之后，在写请求完成后调用
3. 显示设置超时和重试
4. 降级

### 分库分表带来的问题

1. 多维度查询，比如查询前100条数据，但是有100个分库，那么就要查询前1万条数据
2. 满足了写入的ACID，但是在查询的过程中MySql+代理的模式不好满足搜索需要
3. 所以这里到入了ES，注意事项
   1. ES默认最多前10000条数据
   2. 翻页使用游标



## 高性能扣减业务

扣减业务

1. 基于数据库—简单扣减
2. 基于内存—实现单机万级TPS
   1. 扣减保证原子性
   2. redisf的高性能

### 系统详解

#### 高性能扣减业务难点

1. im系统，例如qq或者微博，每个人都读自己的数据（好友列表、群列表、个人信息）
2. 微博系统，每个人读你关注的人的数据，一个人读多个人的数据；
3. 秒杀系统，库存只有一份，所有人会在集中的时间读和写这些数据，多个人读一个数据。

例如：小米手机每周二的秒杀，可能手机只有1万部，但瞬时进入的流量可能是几百几千万。

又例如：12306抢票，票是有限的，库存一份，瞬时流量非常多，都读相同的库存。**读写冲突，锁非常严重，这是秒杀业务难的地方**。那我们怎么优化秒杀业务的架构呢？

#### 优化方向

优化方向有两个（今天就讲这两个点）：

1. **将请求尽量拦截在系统上游**（不要让锁冲突落到数据库上去）。传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的有效流量甚小。以12306为例，一趟火车其实只有2000张票，200w个人来买，基本没有人能买成功，请求有效率为0。
2. **充分利用缓存**，秒杀买票，这是一个典型的读多些少的应用场景，大部分请求是车次查询，票查询，下单和支付才是写请求。一趟火车其实只有2000张票，200w个人来买，最多2000个人下单成功，其他人都是查询库存，写比例只有0.1%，读比例占99.9%，非常适合使用缓存来优化。好，后续讲讲怎么个“将请求尽量拦截在系统上游”法，以及怎么个“缓存”法，讲讲细节。

#### 常见秒杀架构

常见的站点架构基本是这样的（绝对不画忽悠类的架构图）

1. 浏览器端，最上层，会执行到一些JS代码
2. 站点层，这一层会访问后端数据，拼html页面返回给浏览器
3. 服务层，向上游屏蔽底层数据细节，提供数据访问
4. 数据层，最终的库存是存在这里的，mysql是一个典型（当然还有会缓存）

#### 各层次优化细节

##### 客户端优化

1. 产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求；
2. JS层面，限制用户在x秒之内只能提交一次请求；

##### 站点层面的请求拦截

1. 在**站点层面**，对uid进行请求计数和去重，一个uid，5秒只准透过1个请求，这样又能拦住99%的for循环请求。
2. 5s只透过一个请求，同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面

#####  服务层来拦截

1. 对于写请求，做请求队列，每次只透有限的写请求去数据层

   + 1w部手机，只透1w个下单请求去db

   + 3k张火车票，只透3k个下单请求去db

   + 如果均成功再放下一批，如果库存不够则队列里的写请求返回“已售完”。 

2. 对于读请求，cache抗，单机抗个每秒10w应该都是没什么问题的。

3. 当然，还有**业务规则上的一些优化**。回想12306所做的，分时分段售票，原来统一10点卖票，现在8点，8点半，9点，...每隔半个小时放出一批：将流量摊匀。

4. 其次，数据粒度的优化：你去购票，对于余票查询这个业务，票剩了58张，还是26张，你真的关注么，其实我们只关心有票和无票？流量大的时候，做一个粗粒度的“有票”“无票”缓存即可。

5. 一些业务逻辑的异步：例如下单业务与 支付业务的分离。

##### 数据库层

原则上：数据库仅仅去处理有意义的请求

库存是有限的，透这么多请求来数据库没有意义。100w个下单，0个成功，请求有效率0%。透3k个到数据，全部成功，请求有效率100%。

 

#### 总结

两个架构优化思路：

1. **尽量将请求拦截在系统上游**（越上游越好）；
2. **读多写少的常用多使用缓存**（缓存抗读压力）；

方案情况：

1. 浏览器和APP：做限速
2. 站点层：按照uid做限速，做页面缓存
3. 服务层：按照业务做写请求队列控制流量，做数据缓存
4. 数据层：闲庭信步
5. 并且：结合业务做优化

 

#### Q&A

**1**、按你的架构，其实压力最大的反而是站点层，假设真实有效的请求数有1000万，不太可能限制请求连接数吧，那么这部分的压力怎么处理？

答：每秒钟的并发可能没有1kw，假设有1kw，解决方案2个：

1. 站点层是可以通过加机器扩容的，最不济1k台机器来呗。
2. 如果机器不够，抛弃请求，抛弃50%（50%直接返回稍后再试），原则是要保护系统，不能让所有用户都失败。

 

**2**、“控制了10w个肉鸡，手里有10w个uid，同时发请求”这个问题怎么解决哈？

答：上面说了，服务层写请求队列控制

 

**3、**限制访问频次的缓存，是否也可以用于搜索？例如A用户搜索了“手机”，B用户搜索“手机”，优先使用A搜索后生成的缓存页面？

答：这个是可以的，这个方法也经常用在“动态”运营活动页，例如短时间推送4kw用户app-push运营活动，做页面缓存。

 

**4、**如果队列处理失败，如何处理？肉鸡把队列被撑爆了怎么办？

答：处理失败返回下单失败，让用户再试。队列成本很低，爆了很难吧。最坏的情况下，缓存了若干请求之后，后续请求都直接返回“无票”（队列里已经有100w请求了，都等着，再接受请求也没有意义了）

 

**5、**站点层过滤的话，是把uid请求数单独保存到各个站点的内存中么？如果是这样的话，怎么处理多台服务器集群经过负载均衡器将相同用户的响应分布到不同服务器的情况呢？还是说将站点层的过滤放到负载均衡前？

答：可以放在内存，这样的话看似一台服务器限制了5s一个请求，全局来说（假设有10台机器），其实是限制了5s 10个请求，解决办法：

1. 加大限制（这是建议的方案，最简单）
2. 在nginx层做7层均衡，让一个uid的请求尽量落到同一个机器上

 

**6、**服务层过滤的话，队列是服务层统一的一个队列？还是每个提供服务的服务器各一个队列？如果是统一的一个队列的话，需不需要在各个服务器提交的请求入队列前进行锁控制？

答：可以不用统一一个队列，这样的话每个服务透过更少量的请求（总票数/服务个数），这样简单。统一一个队列又复杂了。

 

**7、**秒杀之后的支付完成，以及未支付取消占位，如何对剩余库存做及时的控制更新？

答：数据库里一个状态，未支付。如果超过时间，例如45分钟，库存会重新会恢复（大家熟知的“回仓”），给我们抢票的启示是，开动秒杀后，45分钟之后再试试看，说不定又有票哟~

 

**8、**不同的用户浏览同一个商品落在不同的缓存实例显示的库存完全不一样 请问老师怎么做缓存数据一致或者是允许脏读？

答：目前的架构设计，请求落到不同的站点上，数据可能不一致（页面缓存不一样），这个业务场景能接受。但数据库层面真实数据是没问题的。

 

**9、**就算处于业务把优化考虑3k张火车票，只透3k个下单请求去db那这3K个订单就不会发生拥堵了吗？

1. 数据库抗3k个写请求还是ok的；
2. 可以数据拆分；
3. 如果3k扛不住，服务层可以控制透过去的并发数量，根据压测情况来吧，3k只是举例；

 

**10、**如果在站点层或者服务层处理后台失败的话，需不需要考虑对这批处理失败的请求做重放？还是就直接丢弃？

答：别重放了，返回用户查询失败或者下单失败吧，架构设计原则之一是“fail fast”。

 

**11、**对于大型系统的秒杀，比如12306，同时进行的秒杀活动很多，如何分流？

答：垂直拆分

 

**12**、额外又想到一个问题。这套流程做成同步还是异步的？如果是同步的话，应该还存在会有响应反馈慢的情况。但如果是异步的话，如何控制能够将响应结果返回正确的请求方？

答：用户层面肯定是同步的（用户的http请求是夯住的），服务层面可以同步可以异步。



**13**、秒杀群提问：减库存是在那个阶段减呢？如果是下单锁库存的话，大量恶意用户下单锁库存而不支付如何处理呢？

答：数据库层面写请求量很低，还好，下单不支付，等时间过完再“回仓”，之前提过了。



## 微服务架构

### 设计原则

1. 防备上游
2. 做好自己
3. 怀疑下游

### 新接口发布

1. 微服务中提供2个接口
2. 推动所有外部调用修改后的接口
3. 确认老接口没有调用后，下线老接口